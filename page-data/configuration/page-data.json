{"componentChunkName":"component---src-pages-configuration-index-mdx","path":"/configuration/","result":{"pageContext":{"frontmatter":{"title":"Solution configuration","description":"Configuration for the use case"},"relativePagePath":"/configuration/index.mdx","titleType":"append","MdxNode":{"id":"b95e9c96-2e95-5375-aa30-154475d2c2c6","children":[],"parent":"8bf76fa2-e9ad-5f1e-bcda-dfaff7e1443e","internal":{"content":"---\ntitle: Solution configuration\ndescription: Configuration for the use case\n---\n\nThis chapter documents the environment pre-requisites to deploy the artifacts supporting the integration of WML with ODM.\n\n## Environment\nIn order to run the use case, you will need to have an OpenShift cluster with the following components installed:\n- IBM Cloud Pak for Data 3.0.1\n- IBM Cloud Pak for Automation version 20.0.2\n    - Operational Decision Manager (ODM)\n    - Business Automation Insights (BAI)\n\n## Configuration\n<InlineNotification kind=\"warning\">\nWork in progress\n</InlineNotification>\n\n## Deployment\n\n### WML scoring service deployment\nIn order to run a scoring service from ODM, you need to create and deploy this scoring service to WML. The easiest way to achieve this is to create a service using the AutoAI capability of CP4D, following these steps:\n\n- Open a CP4D project and click the `Add to project` button, then select the `AutoAI experiment` asset type.\n- Provide a name for you experiment, e.g. `AutoAI GCR`.\n- On the next screen, when prompted for a data source, drag and drop the training data CSV file `gcr-training-data.csv` from the [`ml-training-data`](https://github.com/ibm-cloud-architecture/odm-cp4d-integration/tree/master/data/ml-training-data) folder to the `Add data source` pane. The data is then loaded and the available data columns are displayed.\n- In `Select prediction column`, select the `Risk` column. At this point, you should see the following:\n![](../images/autoai-experiment-setting.png)\n- Click the `Run experiment` button. When the generation of alternative pipelines complete, select one of them and click `Save as > Model`, then save the model.\n![](../images/save-pipeline.png)\n- When prompted by a green message box, click `View in project` and on the model page, click the `Promote to deployment space` link.\n- From the main menu (hamburger on the left), you can select `Analytics deployment` then select the deployment space you used to deploy your newly minted scoring model. You should now see something like this:\n![](../images/deployment-space.png)\n- Click on your model, then on the next page, click the `Create deployment` button. Select `Online` for the deployment type, provide a name for the deployment and click `Create`. After a while, your scoring service is now deployed as a web service. Click on the service to get the endpoint to execute the scoring.\n*Take note of this endpoint* as it is the one that you will provide to the ODM decision service so it can invoke the scoring service.\n\nYou can also perform some quick, interactive tests of the scoring service from this page.\n\n![](../images/deployed-service-details.png)\n\n### ODM decision services deployment\n\nAs mentioned in the use case definition, there are several possible approaches to the design of the object model for the rules. Two decision service archives, a *hierarchical* and *flat* version, are available under the [`exports`](https://github.com/ibm-cloud-architecture/odm-cp4d-integration/tree/master/data/exports) folder.\nYou can load them in ODM Decision Center by following these steps:\n- Open the Decision Center business console and click on the `Library` tab.\n- Select the `Import Decision Service` button, navigate to the `dc-export - risk-assessment-main.zip` archive in the `exports` folder and click the `Import` button. Repeat for the same process for the `dc-export - dynamic-risk-assessment-main.zip` archive.\nThe decision service projects are now loaded:\n![](../images/loaded-decision-services.png)\n\n\n\n","type":"Mdx","contentDigest":"286d4d5c831d3dcb61c6a221a05ca96b","counter":121,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Solution configuration","description":"Configuration for the use case"},"exports":{},"rawBody":"---\ntitle: Solution configuration\ndescription: Configuration for the use case\n---\n\nThis chapter documents the environment pre-requisites to deploy the artifacts supporting the integration of WML with ODM.\n\n## Environment\nIn order to run the use case, you will need to have an OpenShift cluster with the following components installed:\n- IBM Cloud Pak for Data 3.0.1\n- IBM Cloud Pak for Automation version 20.0.2\n    - Operational Decision Manager (ODM)\n    - Business Automation Insights (BAI)\n\n## Configuration\n<InlineNotification kind=\"warning\">\nWork in progress\n</InlineNotification>\n\n## Deployment\n\n### WML scoring service deployment\nIn order to run a scoring service from ODM, you need to create and deploy this scoring service to WML. The easiest way to achieve this is to create a service using the AutoAI capability of CP4D, following these steps:\n\n- Open a CP4D project and click the `Add to project` button, then select the `AutoAI experiment` asset type.\n- Provide a name for you experiment, e.g. `AutoAI GCR`.\n- On the next screen, when prompted for a data source, drag and drop the training data CSV file `gcr-training-data.csv` from the [`ml-training-data`](https://github.com/ibm-cloud-architecture/odm-cp4d-integration/tree/master/data/ml-training-data) folder to the `Add data source` pane. The data is then loaded and the available data columns are displayed.\n- In `Select prediction column`, select the `Risk` column. At this point, you should see the following:\n![](../images/autoai-experiment-setting.png)\n- Click the `Run experiment` button. When the generation of alternative pipelines complete, select one of them and click `Save as > Model`, then save the model.\n![](../images/save-pipeline.png)\n- When prompted by a green message box, click `View in project` and on the model page, click the `Promote to deployment space` link.\n- From the main menu (hamburger on the left), you can select `Analytics deployment` then select the deployment space you used to deploy your newly minted scoring model. You should now see something like this:\n![](../images/deployment-space.png)\n- Click on your model, then on the next page, click the `Create deployment` button. Select `Online` for the deployment type, provide a name for the deployment and click `Create`. After a while, your scoring service is now deployed as a web service. Click on the service to get the endpoint to execute the scoring.\n*Take note of this endpoint* as it is the one that you will provide to the ODM decision service so it can invoke the scoring service.\n\nYou can also perform some quick, interactive tests of the scoring service from this page.\n\n![](../images/deployed-service-details.png)\n\n### ODM decision services deployment\n\nAs mentioned in the use case definition, there are several possible approaches to the design of the object model for the rules. Two decision service archives, a *hierarchical* and *flat* version, are available under the [`exports`](https://github.com/ibm-cloud-architecture/odm-cp4d-integration/tree/master/data/exports) folder.\nYou can load them in ODM Decision Center by following these steps:\n- Open the Decision Center business console and click on the `Library` tab.\n- Select the `Import Decision Service` button, navigate to the `dc-export - risk-assessment-main.zip` archive in the `exports` folder and click the `Import` button. Repeat for the same process for the `dc-export - dynamic-risk-assessment-main.zip` archive.\nThe decision service projects are now loaded:\n![](../images/loaded-decision-services.png)\n\n\n\n","fileAbsolutePath":"/home/runner/work/odm-cp4d-integration/odm-cp4d-integration/docs/src/pages/configuration/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}